{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628078c0-8ce6-4930-99a3-fc34e3f9e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import script_automate.train_CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense, InputLayer, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bba8a9-2cc6-4e7b-8b89-fa79661d381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muell\\Documents\\Programmieren\\GitHub\\neural-network-digital-counter-readout\n",
      "C:\\Users\\Muell\\AppData\\Roaming\\jupyter\\runtime\\kernel-d4814e2b-1b78-4cc5-a52c-51c782b8ae70.json\n",
      "C:\\Users\\Muell\\Documents\\Programmieren\\GitHub\\neural-network-digital-counter-readout\\__file__\n",
      "C:\\Users\\Muell\\Documents\\Programmieren\\GitHub\\neural-network-digital-counter-readout\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(os.getcwd())\n",
    "print(sys.argv[2])\n",
    "print(os.path.realpath('__file__'))\n",
    "print(os.path.dirname(os.path.realpath('__file__')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d9de93-5f71-4a83-9188-383fa148b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultName = \"dig\"\n",
    "\n",
    "Input_dir='ziffer_sortiert_resize'\n",
    "Training_Percentage = 0.2\n",
    "\n",
    "### Image Augmentation\n",
    "Shift_Range = 1\n",
    "Brightness_Range = 0.3\n",
    "Rotation_Angle = 5\n",
    "ZoomRange = 0.2\n",
    "\n",
    "Batch_Size = 4\n",
    "\n",
    "Epoch_Anz  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eebd127-ac1b-4636-9263-9d1edd26f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workfile does not exists - init model necessary\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization (BatchNo (None, 32, 20, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 20, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                5643      \n",
      "=================================================================\n",
      "Total params: 324,631\n",
      "Trainable params: 324,625\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DateNow = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "model = script_automate.train_CNN.load_model(DefaultName, DateNow)\n",
    "\n",
    "if model == None:\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(32,20,3)))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation=\"relu\"))\n",
    "    model.add(Dense(11, activation = \"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss= tf.keras.losses.categorical_crossentropy, \n",
    "              optimizer= tf.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95), \n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcba119c-4b2e-4e91-bbe7-0eb8ddcfcda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(0,)\n",
      "(0,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23560/779617357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAnzahlBilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscript_automate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_CNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_load_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTraining_Percentage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Programmieren\\GitHub\\neural-network-digital-counter-readout\\script_automate\\train_CNN.py\u001b[0m in \u001b[0;36mtrain_load_image\u001b[1;34m(_input_dir, _Training_Percentage)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mTraining_Percentage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, \n\u001b[0m\u001b[0;32m     79\u001b[0m                                                         test_size=_Training_Percentage)\n\u001b[0;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\anaconda-win11-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2422\u001b[1;33m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[0;32m   2423\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2424\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\anaconda-win11-env\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2098\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2099\u001b[0m             \u001b[1;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2100\u001b[0m             \u001b[1;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, AnzahlBilder = script_automate.train_CNN.train_load_image(Input_dir, Training_Percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcfd37-d906-4d51-8cd8-3912b3ccdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, validation_iterator = script_automate.train_CNN.get_iterator(x_train, x_test, y_train, y_test, \n",
    "                                                                             Shift_Range, \n",
    "                                                                             Brightness_Range, \n",
    "                                                                             Rotation_Angle, \n",
    "                                                                             ZoomRange, \n",
    "                                                                             Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d728b-1c53-475e-8fef-0921faf53fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_iterator, \n",
    "                    validation_data = validation_iterator, \n",
    "                    epochs          = Epoch_Anz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d50bc5-aa6f-4a89-9325-ede9d07d000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## H5-Format\n",
    "model.save('saved_model/' + DefaultName)\n",
    "model.save('saved_model/' + DefaultName + \"_\" + DateNow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78944bc3-6f5c-44f4-a416-66e884358dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter    = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(DefaultName + \".tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5baf5-7707-43ff-86ba-0a1025ffe771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "FileName = DefaultName + \"_q.tflite\"\n",
    "\n",
    "def representative_dataset():\n",
    "    for n in range(x_train[0].size):\n",
    "      data = np.expand_dims(x_train[5], axis=0)\n",
    "      yield [data.astype(np.float32)]\n",
    "        \n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter2.representative_dataset = representative_dataset\n",
    "converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter2.representative_dataset = representative_dataset\n",
    "tflite_quant_model = converter2.convert()\n",
    "\n",
    "open(FileName, \"wb\").write(tflite_quant_model)\n",
    "print(FileName)\n",
    "Path(FileName).stat().st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f9a55-85fe-461c-9eb3-7a7e2ee7aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('training_' + DefaultName + '.txt', 'a')\n",
    "for x in range(np.size(history.history['loss'])):\n",
    "    text = DateNow + \"\\t\" + str(AnzahlBilder) + \"\\t\"+ str(x+1) + \"\\t\" + str(history.history['loss'][x]) + \"\\t\" + str(history.history['val_loss'][x])\n",
    "    print(text)\n",
    "    file_object.write(text + \"\\n\")\n",
    "    \n",
    "file_object.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
